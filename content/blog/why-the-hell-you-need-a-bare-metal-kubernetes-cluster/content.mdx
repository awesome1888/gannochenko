---
title: 'Why the hell you need a bare-metal Kubernetes cluster?'
description:
keywords:
path: /blog/why-the-hell-you-need-a-bare-metal-kubernetes-cluster-
date: 2020-06-02
published: false
images:
    - author: Toni Vuohelainen
      image: ./cover.jpg
      sourceText: Flickr
      source: https://www.flickr.com/photos/tonivuohelainen/
      is_cover: 1
      galleryId: 0
---

A while ago, being on sick leave, I challenged myself with setting up a bare-metal K8s cluster. I chose `Kubeadm` back then, and so now I am sharing the results.

Yeh, maybe you already have the following question popped up in your mind:

![Why the hell](./why-the-hell.jpg)

Well, you may want to consider setting up a bare-metal cluster, because

üëâ you are setting up an internal infrastructure for your company, and using cloud providers is not an option,<br />
üëâ you wish to receive close-to-production experience with you own pet K8s,<br />
üëâ you are well-aware of Minikube, MicroK8s, etc, but always wanted to have a real multi-node cluster out there,<br />
üëâ you don't want to pay for a pre-configured AKS/GKE/EKS/whatever solution just yet, or not allowed to use it,<br />
üëâ you want to set everything up from scratch, at least once, out of curiosity,<br />
üëâ you have free time :)<br />

Why making another article while there is plenty of them already? Well because most of the material is obviously not for beginners.
Sometimes the material feels fragmented, ripped ouf of the context of something bigger or simply not covering the entire process.

What you should understand just before we continue.

‚òùÔ∏è It still won't be a production-ready environment for mature projects! üö®üö®üö®<br />
‚òùÔ∏è However, with extra hours dedicated, the cluster can become production-ready.<br />
‚òùÔ∏è But even so, it will be good enough for hobby projects, proof of concepts, learning and exploration.<br />
‚òùÔ∏è You will not receive any GUI control panel, only raw `kubectl`.<br />

Resonates with your needs? Then let's go!

## Nodes

Kubernetes is a `cluster`, it means that typically more than one machine is involved.
I got for myself two instances of VDS (VPS), with 2 Cores and 4 GB of RAM each (those are the minimum requirements), Centos7 on-board. It was a cheap russian hosting with a data center in Moscow and unlimited bandwidth.
It works for me just fine, but you can pick AWS's EC2, DigitalOcean or Vultr, or any other provider.

You may as well get two virtual machines with VirtualBox and Vagrant, but it will not give you the same experience.

* `X.X.X.X` - IP address of Node A
* `Y.Y.Y.Y` - IP address of Node B

Just as a part of pre-flight preparations, I run the following commands to make the whole process a bit easier.

On my machine:

~~~bash
printf "\nX.X.X.X k8s-master\nY.Y.Y.Y k8s-node01\n" | sudo tee -a /etc/hosts;
scp ~/.ssh/id_rsa.pub root@X.X.X.X:~
scp ~/.ssh/id_rsa.pub root@Y.Y.Y.Y:~
~~~

On each node under `root`:

~~~bash bashRoot
yum update

# create a non-root user
useradd admin

# enable sudo
usermod -aG wheel admin
passwd admin
passwd -l admin

# enable key-based sign-in
cd /home/admin/
mkdir ./.ssh
chmod 711 ./.ssh
cat ~/id_rsa.pub >> ./.ssh/authorized_keys
chmod 600 ./.ssh/authorized_keys
chown -R admin:admin ./.ssh
~~~

You may also want to disable password-based SSH authentication, disable root login and switch SSH to a different port to ward off brute-forcers.

Now I can sign-in the nodes by typing `ssh admin@k8s-node01` without password and able to run `sudo`.

## Common setup

Kubernetes logically consists of two parts: `Control plane` and `Compute`.

* `Control plane` operates the cluster, and it is hosted on so-called `master node(s)`.
* `Compute` runs containerized applications and is hosted on `worker nodes`.

Therefore, I choose `Node A` to be the `master node`, whereas `Node B` - `worker node`.

The following set of commands I need to run <ins>on each node</ins>:

‚û°Ô∏è Allow nodes to address each other by name, so I don't hard-code IP addresses:

~~~bash
printf "\nX.X.X.X master\nY.Y.Y.Y node01\n" | sudo tee -a /etc/hosts;
~~~

Make sure nodes can see each other:

~~~bash
ping master
ping node01
~~~

‚û°Ô∏è K8s does not play nicely with SELinux, switching it off:

~~~bash
sudo setenforce 0
sudo sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
~~~

‚û°Ô∏è K8s uses `br_netfilter` kernel module for it's internal networking. So I check if module even exists, load it, and enable on-boot load: 

~~~bash
sudo modprobe br_netfilter
sudo echo "br_netfilter" | sudo tee -a /etc/modules-load.d/br_netfilter.conf
~~~

‚û°Ô∏è Tell the `bridge module` to use `iptables`:

~~~bash
echo '1' | sudo tee -a /proc/sys/net/bridge/bridge-nf-call-iptables
printf "\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward=1\n" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
~~~

‚û°Ô∏è Some default firewall rules are set in a way it paralyzes cluster networking. In production it totally makes sense to find what causes that and fix the issue. Since I am building a non-production environment, I don't bother investing time. So I just disable `firewalld` and by doing so I wipe out all firewall rules.

~~~bash
sudo systemctl stop firewalld
sudo systemctl disable firewalld
sudo systemctl mask --now firewalld

# making sure there is not a single rule left, and the policy is "ACCEPT" on every chain
sudo iptables -L -v -n
~~~

‚û°Ô∏è K8s demands the `swap` partition to be disabled for good:

~~~bash
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab;
~~~

‚û°Ô∏è Install and configure `Docker`

~~~bash
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum install -y docker-ce

# changing docker cgroupdriver to "systemd" 
sudo mkdir /etc/docker
sudo tee /etc/docker/daemon.json > /dev/null <<'EOF'
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF
~~~

‚û°Ô∏è Install `Kubernetes` and `kubeadm`. Yep, `kubeadm` will take care of all the boring stuff here.

~~~bash
sudo tee /etc/yum.repos.d/kubernetes.repo > /dev/null <<'EOF'
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

sudo yum install -y kubelet kubeadm kubectl
~~~

‚û°Ô∏è Start `Docker` and `Kubelet` - a daemon that is in charge of doing all K8s-related stuff on this particular node.

~~~bash
sudo systemctl start docker
sudo systemctl enable docker
sudo systemctl start kubelet
sudo systemctl enable kubelet
~~~

Allright, hopefully no errors there, and we can proceed to more neat stuff.

## Master-specific setup

The commands below should be executed <ins>on the master node only</ins>.


‚û°Ô∏è Let `kubeadm` do all the heavy lifting:
~~~bash
sudo kubeadm init --apiserver-advertise-address=X.X.X.X --apiserver-cert-extra-sans=localhost,127.0.0.1;
~~~

If everything went well, I just need to do a few more extra steps.

‚û°Ô∏è Create a configuration file for the current user (admin)

~~~bash
mkdir -p ${HOME}/.kube
sudo cp -i /etc/kubernetes/admin.conf ${HOME}/.kube/config
sudo chown $(id -u):$(id -g) ${HOME}/.kube/config
~~~

‚û°Ô∏è Deploy a pod network.

So we have containers. A `container` is a containerized application written in any language (Node, Java, Go, ...) and running inside of the cluster.
A `pod` is a group of `containers`, which share common resources.
`Pods` are exposed to the outer world via `services`.

Yeh, sounds complicated. But this is what makes K8s quite abstracted and therefore unbiased and flexible.

So the `pod network` then is an implementation of K8s networking model: it enables pods talking to each other, to services, and via services - to the outer world.

There are numerous implementations out there, but I will stick to the kinda default one, which is called `Flannel`. It is basic and proven to be working.

~~~bash
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
~~~

## Worker-specific setup

The commands below should be executed <ins>on the worker node(s) only</ins>.

sudo ./kubeadm_join_cmd.sh

## Foo

* `Container` is exposed via `services`.
* `Services` are exposed via `ingress` and `load balancer`.

## Useful links

* [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/) - K8s's official bootstrapping tool
* [kops](https://kops.sigs.k8s.io/) - K8s installer on AWS/GCE/OpenStack/Digital Ocean/Spot Ocean
* [kubespray](https://kubespray.io/) - another installer
* [Pod networks in the alphabetical order](https://kubernetes.io/docs/concepts/cluster-administration/networking/)
